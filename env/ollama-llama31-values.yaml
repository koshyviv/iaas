# Ollama Configuration for Llama 3.1 8B
# Deployed on worker node 1 (k8s-w-1)

# Image configuration
image:
  repository: ollama/ollama
  tag: "latest"
  pullPolicy: IfNotPresent

# Node placement - pin to specific worker node
nodeSelector:
  kubernetes.io/hostname: k8s-w-1  # CHANGE THIS to your actual worker node name

# Service configuration
service:
  type: ClusterIP
  port: 11434
  targetPort: 11434

# Persistent storage for models
persistentVolume:
  enabled: true
  size: 30Gi  # Llama 3.1 8B needs ~5GB, extra space for other models
  storageClass: ""  # Use default storage class
  accessMode: ReadWriteOnce
  mountPath: /root/.ollama

# Ollama configuration
ollama:
  # Models to pull and run
  models:
    pull: 
      - "llama3.1:8b"
    run:
      - "llama3.1:8b"
  
  # GPU configuration (uncomment for GPU nodes)
  # gpu:
  #   enabled: true
  #   count: 1
  #   type: "nvidia"

# Resource configuration
resources:
  limits:
    cpu: "4"
    memory: "8Gi"
    # nvidia.com/gpu: 1  # Uncomment for GPU
  requests:
    cpu: "1"
    memory: "4Gi"

# Security context
securityContext:
  runAsUser: 0  # Ollama needs root access
  runAsGroup: 0
  fsGroup: 0

containerSecurityContext:
  allowPrivilegeEscalation: true
  readOnlyRootFilesystem: false
  capabilities:
    add:
      - SYS_RESOURCE  # For memory management

# Environment variables
env:
  - name: OLLAMA_HOST
    value: "0.0.0.0"
  - name: OLLAMA_ORIGINS
    value: "*"
  - name: OLLAMA_MODELS
    value: "/root/.ollama/models"
  - name: OLLAMA_KEEP_ALIVE
    value: "5m"
  - name: OLLAMA_MAX_LOADED_MODELS
    value: "3"
  - name: OLLAMA_NUM_PARALLEL
    value: "4"
  - name: OLLAMA_MAX_QUEUE
    value: "512"

# Health checks
livenessProbe:
  httpGet:
    path: /api/tags
    port: 11434
  initialDelaySeconds: 120  # Model loading can take time
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 5

readinessProbe:
  httpGet:
    path: /api/tags
    port: 11434
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Startup probe for initial model loading
startupProbe:
  httpGet:
    path: /api/tags
    port: 11434
  initialDelaySeconds: 30
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 20  # Allow up to 10 minutes for startup

# Service account
serviceAccount:
  create: true
  name: ""
  automountServiceAccountToken: false

# RBAC
rbac:
  create: true

# Pod disruption budget
podDisruptionBudget:
  enabled: true
  minAvailable: 0  # Allow disruption for maintenance

# Tolerations for dedicated ML nodes
tolerations: []
# - key: "ml-workload"
#   operator: "Equal"
#   value: "true"
#   effect: "NoSchedule"

# Affinity rules
affinity:
  # Prefer nodes with local SSD
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]

# Network policy
networkPolicy:
  enabled: false

# Custom labels
labels:
  app.kubernetes.io/part-of: "inference-stack"
  app.kubernetes.io/component: "model-backend"
  model: "llama31"

# Custom annotations
annotations:
  model.inference-stack.io/name: "llama3.1:8b"
  model.inference-stack.io/size: "8b"
  model.inference-stack.io/type: "chat"

# Init container to ensure model directory exists
initContainers:
  - name: init-model-dir
    image: busybox:1.36
    command: ['sh', '-c', 'mkdir -p /root/.ollama/models && chown -R 0:0 /root/.ollama']
    volumeMounts:
      - name: data
        mountPath: /root/.ollama
    securityContext:
      runAsUser: 0

# Additional volumes
extraVolumes: []

# Additional volume mounts  
extraVolumeMounts: []

# Service monitor for Prometheus
serviceMonitor:
  enabled: false  # Enable when Prometheus is deployed
  interval: 30s
  path: /metrics
