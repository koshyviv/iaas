# Ollama Configuration for Phi3 Mini
# Deployed on control plane node (k8s-cp-1) for demo purposes

# Image configuration
image:
  repository: ollama/ollama
  tag: "latest"
  pullPolicy: IfNotPresent

# Node placement - pin to control plane node (demo only)
nodeSelector:
  kubernetes.io/hostname: k8s-cp-1  # CHANGE THIS to your actual control plane node name

# Tolerate control plane taints (for demo deployment)
tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule

# Service configuration
service:
  type: ClusterIP
  port: 11434
  targetPort: 11434

# Persistent storage for models
persistentVolume:
  enabled: true
  size: 20Gi  # Phi3 Mini needs ~2GB, extra space for other models
  storageClass: ""  # Use default storage class
  accessMode: ReadWriteOnce
  mountPath: /root/.ollama

# Ollama configuration
ollama:
  # Models to pull and run
  models:
    pull: 
      - "phi3:mini"
    run:
      - "phi3:mini"
  
  # GPU configuration (uncomment for GPU nodes)
  # gpu:
  #   enabled: true
  #   count: 1
  #   type: "nvidia"

# Resource configuration (smaller for mini model)
resources:
  limits:
    cpu: "2"
    memory: "4Gi"
    # nvidia.com/gpu: 1  # Uncomment for GPU
  requests:
    cpu: "500m"
    memory: "2Gi"

# Security context
securityContext:
  runAsUser: 0  # Ollama needs root access
  runAsGroup: 0
  fsGroup: 0

containerSecurityContext:
  allowPrivilegeEscalation: true
  readOnlyRootFilesystem: false
  capabilities:
    add:
      - SYS_RESOURCE  # For memory management

# Environment variables
env:
  - name: OLLAMA_HOST
    value: "0.0.0.0"
  - name: OLLAMA_ORIGINS
    value: "*"
  - name: OLLAMA_MODELS
    value: "/root/.ollama/models"
  - name: OLLAMA_KEEP_ALIVE
    value: "5m"
  - name: OLLAMA_MAX_LOADED_MODELS
    value: "2"
  - name: OLLAMA_NUM_PARALLEL
    value: "2"
  - name: OLLAMA_MAX_QUEUE
    value: "256"

# Health checks
livenessProbe:
  httpGet:
    path: /api/tags
    port: 11434
  initialDelaySeconds: 120  # Model loading can take time
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 5

readinessProbe:
  httpGet:
    path: /api/tags
    port: 11434
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Startup probe for initial model loading
startupProbe:
  httpGet:
    path: /api/tags
    port: 11434
  initialDelaySeconds: 30
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 15  # Allow up to 7.5 minutes for startup (mini model)

# Service account
serviceAccount:
  create: true
  name: ""
  automountServiceAccountToken: false

# RBAC
rbac:
  create: true

# Pod disruption budget
podDisruptionBudget:
  enabled: true
  minAvailable: 0  # Allow disruption for maintenance

# Affinity rules
affinity:
  # Prefer nodes with local SSD
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values: ["amd64"]

# Network policy
networkPolicy:
  enabled: false

# Custom labels
labels:
  app.kubernetes.io/part-of: "inference-stack"
  app.kubernetes.io/component: "model-backend"
  model: "phi3-mini"

# Custom annotations
annotations:
  model.inference-stack.io/name: "phi3:mini"
  model.inference-stack.io/size: "mini"
  model.inference-stack.io/type: "chat"
  deployment.inference-stack.io/note: "Demo deployment on control plane"

# Init container to ensure model directory exists
initContainers:
  - name: init-model-dir
    image: busybox:1.36
    command: ['sh', '-c', 'mkdir -p /root/.ollama/models && chown -R 0:0 /root/.ollama']
    volumeMounts:
      - name: data
        mountPath: /root/.ollama
    securityContext:
      runAsUser: 0

# Additional volumes
extraVolumes: []

# Additional volume mounts  
extraVolumeMounts: []

# Service monitor for Prometheus
serviceMonitor:
  enabled: false  # Enable when Prometheus is deployed
  interval: 30s
  path: /metrics
