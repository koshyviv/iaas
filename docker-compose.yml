version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    entrypoint: ["/bin/sh", "-lc", "ollama serve & sleep 2 && ollama pull llama3.2:1b || true && wait"]
    restart: unless-stopped

  langfuse-db:
    image: postgres:16-alpine
    container_name: langfuse_db
    environment:
      POSTGRES_DB: langfuse
      POSTGRES_USER: langfuse
      POSTGRES_PASSWORD: langfuse
    volumes:
      - langfuse_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langfuse -d langfuse"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  langfuse:
    image: langfuse/langfuse:latest
    container_name: langfuse
    depends_on:
      langfuse-db:
        condition: service_healthy
    ports:
      - "3101:3000"
    environment:
      DATABASE_URL: postgresql://langfuse:langfuse@langfuse-db:5432/langfuse
      NEXTAUTH_URL: http://localhost:3101
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET:-demo-nextauth-secret}
      SALT: ${SALT:-demo-salt}
    restart: unless-stopped

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    depends_on:
      - ollama
      - langfuse
    ports:
      - "4001:4000"
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-demo-litellm}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-pk-demo}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-sk-demo}
      LANGFUSE_HOST: http://langfuse:3000
    command: --config /app/config.yaml --detailed_debug
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
    restart: unless-stopped

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    depends_on:
      - litellm
    ports:
      - "3000:8080"
    environment:
      # Point OpenWebUI to LiteLLM as an OpenAI-compatible gateway
      OPENAI_API_BASE_URL: http://litellm:4001/v1
      OPENAI_API_KEY: ${LITELLM_MASTER_KEY:-sk-demo-litellm}
    volumes:
      - openwebui_data:/app/backend/data
    restart: unless-stopped

volumes:
  langfuse_db_data:
  openwebui_data:
  ollama_models:


