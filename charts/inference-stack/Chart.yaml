apiVersion: v2
name: inference-stack
description: Complete LLM inference platform with routing, keys, and UI
type: application
version: 0.1.0
appVersion: "1.0.0"
home: https://github.com/your-org/inference-stack
sources:
  - https://github.com/your-org/inference-stack
maintainers:
  - name: Platform Team
    email: platform@yourcompany.com

keywords:
  - llm
  - inference
  - kubernetes
  - ai
  - litellm
  - ollama

dependencies:
  # Load balancer for bare metal
  - name: metallb
    version: "0.15.0"
    repository: https://metallb.github.io/metallb
    condition: metallb.enabled

  # Ingress controller
  - name: ingress-nginx
    version: "4.13.1"
    repository: https://kubernetes.github.io/ingress-nginx
    condition: ingress-nginx.enabled

  # Metrics server for HPA
  - name: metrics-server
    version: "3.13.0"
    repository: https://kubernetes-sigs.github.io/metrics-server
    condition: metrics-server.enabled

  # Database for LiteLLM
  - name: postgresql
    version: "16.7.26"
    repository: https://charts.bitnami.com/bitnami
    alias: postgres
    condition: postgres.enabled

  # Cache for LiteLLM
  - name: redis
    version: "20.5.0"
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled

  # Web UI
  - name: open-webui
    version: "7.3.0"
    repository: https://helm.openwebui.com/
    condition: open-webui.enabled

  # Model backends (Ollama instances)
  - name: ollama
    version: "1.6.2"
    repository: https://cowboysysop.github.io/charts/
    alias: ollama-llama31
    condition: ollama-llama31.enabled

  - name: ollama
    version: "1.6.2"
    repository: https://cowboysysop.github.io/charts/
    alias: ollama-mistral
    condition: ollama-mistral.enabled

  - name: ollama
    version: "1.6.2"
    repository: https://cowboysysop.github.io/charts/
    alias: ollama-phi3
    condition: ollama-phi3.enabled

annotations:
  category: AI/ML
  licenses: Apache-2.0
